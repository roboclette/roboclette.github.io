<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Robolette">
<meta name="author" content="Sylvain Calinon">

<title>Roboclette</title>

<!-- Bootstrap core CSS -->
<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

<!-- Custom fonts for this template
<link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
-->
<link href="https://fonts.googleapis.com/css?family=Open+Sans:700,400,300" rel="stylesheet">

<!-- Custom styles for this template -->
<link href="css/scrolling-nav.css" rel="stylesheet">

</head>

<body id="page-top">


<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="#page-top"><img height="40px" src="images/roboclette-logo01.svg"></a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
<!--
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#project">Project</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#consortium">Consortium</a>
        </li>
      </ul>
    </div>
-->
  </div>
</nav>


<header>
<div class="container text-center">
<img width="80%" src="images/roboclette-logo01.svg" class="img-fluid mx-auto d-block">
</div>
</header>

<section id="video" class="bg-light">
<div class="container">
<div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/w0in0Lr0FbU?cc_load_policy=1&loop=1&modestbranding=1&rel=0&showinfo=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div>
</section>

<section id="project">
<div class="container">
<div class="row">
<div class="col-lg-12 mx-auto">

<h2>When a robot learns how to make traditional Swiss raclette thanks to AI</h2>
<p>It took the know-how of the cheese Master Eddy Baillifard, founder of Raclett'House, and the expertise of the Robot learning & interaction group from the Idiap research institute to achieve the first raclette made by a robot. Beyond the technical challenge, the project shows the potential of learning from demonstration techniques and for a better human-robot collaboration.</p>

<p>The cheese master hand is quick and smooth: he applies a precisely controlled pressure on the melting cheese surface to let it land on the plate. Et voilà, the famous Swiss raclette! The gesture seems easy, but the know-how behind it is crucial. Every raclette enthusiast can testify how hard it is to master this apparently simple skill. “Depending on the type of raclette cheese, the surface can vary in terms of softness and can be more or less fluid,” says Eddy Baillifard, the Master and ambassador of the raclette cheese. Mimicking this skill with a robot isn't easy. To achieve this, Emmanuel Pignat, a PhD from the Idiap's robotic group used a novel approach: Eddy guided the robot's arm to let it record the movements and forces needed to perform the task. This learning technique allows to transfer skills from the human to the robot in an intuitive manner. The range of applications is wide, including industrial robots, service robots, and assistive robots. It relieves professionals of their most repetitive, dangerous or uncomfortable duties, in order to spend more time using their human skills, such as supervision, evaluation or decision making.</p>

<h2>A smart raclette</h2>

<p>If learning from demonstration – the scientific term for copying a gesture – seems an obvious task to us humans, the challenge is much harder for robots. They are usually programmed for a specific task, which is repeated accurately in a loop. “Thanks to artificial intelligence algorithms, the robotic arm can generate movements that can adapt to new situations. In this specific case, the cheese can be in slightly different positions and orientations, and there can be more or less cheese left in the oven,” says Sylvain Calinon, head of Idiap's Robot learning & interaction group. “This ability to adapt is the key to allow efficient human-robot collaboration. The cheese Master shows each time a slightly different way to scrape the raclette cheese onto the plate. The underlying learning algorithms allows the robot to integrate these nuances to recreate a meaningful gesture in a new situation.”</p>

<h2>When tradition meets innovation</h2>

<p>This collaboration between the cheese Master and the researchers was supported by the whole Canton of Valais. The local authorities brought their financial support to this scientific research. The TTM company created a specific oven with a handle for the robot to grab the plate with the cheese. Nicolas Fontaine, a young local entrepreneur, facilitated the contacts between TTM, Raclett'House and Valais/Wallis Promotion which made the film. “When Nicolas Fontaine asked us for help, we immediately decided to help. This project highlights our strong belief about Valais: it's a unique territory mixing tradition and innovation. Both are represented by the best people and skills as illustrated in this short film,” says Alessandro Marcolin, marketing director of Valais/Wallis Promotion.</p>

</div>
</div>
</div>
</section>

<!--
<h2>More information</h2>

<p>The learning approach behind this application is described in this scientific publication, with the related video. Another example of application of this approach is to teach robots to assist elderly or disabled people to get dressed, described here, with related video.</p>

The original video is available here.

Partners involved in the project:

Idiap Robot Learning & Interaction group
Raclett’House
Raclette oven TTM
Valais/Wallis Promotion

<section id="publications" class="bg-light">
<div class="container">
<div class="row">
<div class="col-lg-12 mx-auto">
<h2>Publications</h2>
<p>
<ul>
<li>Thomas Duboudin, Maxime Petit and Liming Chen, <a href="https://www.researchgate.net/profile/Maxime_Petit4/publication/333929001_Toward_a_Procedural_Fruit_Tree_Rendering_Framework_for_Image_Analysis/links/5d0cd22892851cf4403eb3c8/Toward-a-Procedural-Fruit-Tree-Rendering-Framework-for-Image-Analysis.pdf" target="_BLANK"><b>Toward a Procedural Fruit Tree Rendering Framework for Image Analysis</b></a>, <i>7th Int. Workshop on Image Analysis Methods in the Plant Sciences (IAMPS), Lyon (France), 2019</i></li>
    <ul><li><a href='publi/iamps2019/IAMPS_2019_poster.pdf'>Poster</a> and <a href='publi/iamps2019/IAMPS_2019_presentation.pdf'>Oral Presentation</a> </li>
    <li>Code: <a href='https://github.com/tduboudi/IAMPS2019-Procedural-Fruit-Tree-Rendering-Framework'>https://github.com/tduboudi/IAMPS2019-Procedural-Fruit-Tree-Rendering-Framework</a></li>
    </ul>
</ul>
</section>


<section id="consortium">
<div class="container">
<div class="row">
	<div class="col-lg-12 mx-auto">
	<h2>Consortium</h2>
	<div class="container">
	<div class="row">
		<div class="col-lg-4 text-center my-auto">
		<a href="http://www.idiap.ch/en" target="_BLANK"><img src="images/Idiap.png" class="img-fluid"></a>
		</div>
		<div class="col-lg-4 text-center my-auto">
		<a href="https://www.iit.it/" target="_BLANK"><img src="images/IIT.png" class="img-fluid"></a>
		</div>
		<div class="col-lg-4 text-center my-auto">
		<a href="https://www.ec-lyon.fr/en" target="_BLANK"><img src="images/ECL.png" class="img-fluid"></a>
		</div>
	</div>
	<div class="row">
		<div class="col-lg-4 text-center my-auto">
		<a href="http://www.idiap.ch/en" target="_BLANK"><h5 class="mb-3">Idiap Research Institute</h5></a>
		</div>
		<div class="col-lg-4 text-center my-auto">
		<a href="https://www.iit.it/" target="_BLANK"><h5 class="mb-3">Italian Institute of Technology</h5></a>
		</div>
		<div class="col-lg-4 text-center my-auto">
		<a href="https://www.ec-lyon.fr/en" target="_BLANK"><h5 class="mb-3">École Centrale de Lyon</h5></a>
		</div>
	</div>
	<div class="row">
		<div class="col-lg-4 text-center">
		<p class="text-muted mb-0">Members:</p>
		<p class="text-muted mb-0"><a href="http://calinon.ch/" target="_BLANK">Sylvain Calinon</a></p>
		<p class="text-muted mb-0"><a href="http://andreanjos.org/" target="_BLANK">André Anjos</a></p>
		<p class="text-muted mb-0"><a href="http://www.idiap.ch/~marcel/professional/Welcome.html" target="_BLANK">Sébastien Marcel</a></p>
		</div>
		<div class="col-lg-4 text-center">
		<p class="text-muted mb-0">Members:</p>
		<p class="text-muted mb-0"><a href="https://www.linkedin.com/in/fei-chen-55662388/?originalSubdomain=it" target="_BLANK">Fei Chen</a></p>
		<p class="text-muted mb-0"><a href="https://www.linkedin.com/in/andreaincertidelmonte/" target="_BLANK">Andrea Incerti Delmonte</a></p>
		</div>
		<div class="col-lg-4 text-center">
		<p class="text-muted mb-0">Members:</p>
		<p class="text-muted mb-0"><a href="http://perso.ec-lyon.fr/liming.chen/" target="_BLANK">Liming Chen</a></p>
		<p class="text-muted mb-0"><a href="https://www.researchgate.net/profile/Maxime_Petit4" target="_BLANK">Maxime Petit</a></p>
		</div>
	</div>

	</div>
	</div>
</div>
</div>
</section>
-->



<!-- Footer <small><pre class="text-light" > -->
<!--
<footer class="py-5 bg-dark">
<div class="container">
<p class="text-light">
Learn-Real is a collaborative project supported by <a href="http://www.snf.ch/en/Pages/default.aspx" target="_BLANK">SNSF</a> (Switzerland), <a href="http://www.miur.gov.it/" target="_BLANK">MIUR</a> (Italy), and <a href="http://www.agence-nationale-recherche.fr/en/" target="_BLANK">ANR</a> (France), through the <a href="http://www.chistera.eu/call-2017-announcement" target="_BLANK">ERA-NET CHIST-ERA 2017 Call ORMR</a> (Object recognition and manipulation by robots: Data sharing and experiment reproducibility).
</p>
<p class="text-muted">
The Learn-Real logo can be generated from a Linux command line (if the Candice font is not available on your system, it can be replaced by another font):
</p>
<p style="font-size: x-small; line-height: 80%;"><code>
convert -size 400x400 -virtual-pixel tile xc: +noise Random -blur 0x1 -channel G -evaluate add 40% -separate pattern.png<br>
convert -size 1720x520 xc:white -font Candice -pointsize 288 -stroke black -strokewidth 100 -annotate +100+344 'Learn-Real' -blur 0x32 -resize 25% -ordered-dither o8x8,6 -resize 400% background.png<br>
convert -size 1720x520 xc:none -font Candice -pointsize 288 -fill 'cmyka(100%,0%,0%,0%,0.8)' -stroke 'cmyka(100%,0%,0%,0%,0.8)' -strokewidth 40 -annotate +88+340 'Learn-Real' -channel RGBA -wave 8x40 c_layer.png<br>
convert -size 1720x520 xc:none -font Candice -pointsize 288 -fill 'cmyka(0%,100%,0%,0%,0.8)' -stroke 'cmyka(0%,100%,0%,0%,0.8)' -strokewidth 40 -annotate +112+340 'Learn-Real' -channel RGBA -wave 8x40 m_layer.png<br>
convert -size 1720x520 xc:none -font Candice -pointsize 288 -fill 'cmyka(0%,0%,100%,0%,0.8)' -stroke 'cmyka(0%,0%,100%,0%,0.8)' -strokewidth 40 -annotate +100+340 'Learn-Real' -channel RGBA -rotate -90 -wave 8x40 -rotate +90 y_layer.png<br>
convert -size 1720x520 xc:none -font Candice -pointsize 288 -fill tile:pattern.png -stroke black -strokewidth 20 -annotate +100+344 'Learn-Real' -stroke none -annotate +100+344 'Learn-Real' -ordered-dither o8x8,6 foreground.png<br>
convert \( c_layer.png null: m_layer.png -compose multiply -layers Composite \) null: y_layer.png -compose multiply -layers Composite cmy_layer.png<br>
convert \( background.png null: cmy_layer.png -layers Composite \) null: foreground.png -layers Composite logo.png
</code></p>
<p class="text-muted">
The use of a command line to generate the logo evokes the synthetic image rendering and reproducibility aspects of the project. It also alludes to the exploitation of existing tools in new contexts, as we do in Learn-Real through the use of the Beat platform for the benchmarking of robot manipulation tasks. The white noise texture and the superposed CMYK colored layers allude to the variational aspects, which are central to the learning aspects of the project. The dithered background evokes the rendering techniques employed in early video games, referring to the use in the project of engines (simulators) and interfaces (head-mounted displays) originating from the video game industry.
</p>
</div>
</footer>
-->


<!-- Bootstrap core JavaScript -->
<script src="vendor/jquery/jquery.min.js"></script>
<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Plugin JavaScript -->
<script src="vendor/jquery-easing/jquery.easing.min.js"></script>

<!-- Custom JavaScript for this theme -->
<script src="js/scrolling-nav.js"></script>

</body>
</html>
